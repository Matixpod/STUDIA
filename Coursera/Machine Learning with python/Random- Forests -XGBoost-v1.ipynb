{"cells":[{"cell_type":"markdown","id":"94ef5300-5fc6-46dc-a1a6-23dec5611278","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Lab: Comparing Random Forest and XGBoost modeling performance\n","Estimated time needed: **20** minutes\n","    \n","\n","## Objectives\n","\n","\n","After completing this lab, you will be able to:\n","\n","* Use scikit-learn to implement Random Forest and XGBoost regression models\n","* Compare the performances of the two models \n","\n"]},{"cell_type":"markdown","id":"d54a6bdc-7ae8-42e8-bf9e-52262a64ceda","metadata":{},"outputs":[],"source":["## Introduction\n","In this lab, you'll create and measure the relative performances of Random Forest and XGBoost regression models for predicting house prices using the California Housing Dataset.\n","'Performance' means both speed and accuracy.\n"]},{"cell_type":"markdown","id":"f961fe51-ab5c-4509-8dbf-bbeb5286adc8","metadata":{},"outputs":[],"source":["First, we need to install the libraries that will be required in this lab.\n"]},{"cell_type":"code","id":"ad0f4304-122a-4617-949d-df3e388a5392","metadata":{},"outputs":[],"source":["!pip install numpy==2.2.0\n!pip install scikit-learn==1.6.0\n!pip install matplotlib==3.9.3\n!pip install xgboost==2.1.3"]},{"cell_type":"markdown","id":"311f355e-57b1-490d-b3dc-a6daf0e36e4f","metadata":{},"outputs":[],"source":["You may now start by importing the required libraries:\n"]},{"cell_type":"code","id":"449feeab-64ff-4570-b314-459c707437ab","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport time"]},{"cell_type":"code","id":"82ff24d0-d1f6-4585-bbb2-a966a6389401","metadata":{},"outputs":[],"source":["# Load the California Housing dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","id":"0ce01166-850e-44f8-a1e8-198d62ce1bae","metadata":{},"outputs":[],"source":["### Exercise 1: How many observations and features does the dataset have?\n"]},{"cell_type":"code","id":"bb2a1a03-d078-4d4f-be98-eb4816547c35","metadata":{},"outputs":[],"source":["# Enter your code here\nN_observations, N_features = \nprint('Number of Observations: ' + str(N_observations))\nprint('Number of Features: ' + str(N_features))\n"]},{"cell_type":"markdown","id":"44f5a974-011f-4e6f-904e-501ff8e24634","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","N_observations, N_features = X.shape\n","print('Number of Observations: ' + str(N_observations))\n","print('Number of Features: ' + str(N_features))\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"ef610cf0-f6b5-4267-ae0c-f4482fdaf962","metadata":{},"outputs":[],"source":["This is a moderately sized dataset used for this analysis.  \n","Keep in mind you are only using one dataset so you have to consider that the comparison may change with scale.\n"]},{"cell_type":"markdown","id":"ddcd08a8-225f-4768-8600-2448495e11de","metadata":{},"outputs":[],"source":["### Initialize models\n","In this step you define the number of base estimators, or individual trees, to be used in each model, and then intialize models for Random Forest regression and XGBoost regression.  You'll just use the default parameters to make the performance comparisons. As a part of the performance comparison, we'll also measure the training times for both models.\n"]},{"cell_type":"code","id":"23149d9c-8ef7-43ee-a75a-295ab558b6ce","metadata":{},"outputs":[],"source":["# Initialize models\nn_estimators=100\nrf = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\nxgb = XGBRegressor(n_estimators=n_estimators, random_state=42)"]},{"cell_type":"code","id":"44e7e89b-ea0c-4e80-a57d-47333005f286","metadata":{},"outputs":[],"source":["# Fit models\n# Measure training time for Random Forest\nstart_time_rf = time.time()\nrf.fit(X_train, y_train)\nend_time_rf = time.time()\nrf_train_time = end_time_rf - start_time_rf\n\n# Measure training time for XGBoost\nstart_time_xgb = time.time()\nxgb.fit(X_train, y_train)\nend_time_xgb = time.time()\nxgb_train_time = end_time_xgb - start_time_xgb"]},{"cell_type":"markdown","id":"a08177d5-bcf0-4a94-a964-d1c617a52f2e","metadata":{},"outputs":[],"source":["### Exercise 2. Use the fitted models to make predictions on the test set. \n","Also, measure the time it takes for each model to make its predictions using the time.time() function to measure the times before and after each model prediction.\n"]},{"cell_type":"code","id":"cd1d0a45-6334-459d-94c2-7fb1a9757c96","metadata":{},"outputs":[],"source":["# Enter your code here\n\n# Measure prediction time for Random Forest\nstart_time_rf = \ny_pred_rf = \nend_time_rf = \nrf_pred_time = \n\n# Measure prediction time for XGBoost\nstart_time_xgb = \ny_pred_xgb = \nend_time_xgb = \nxgb_pred_time = \n"]},{"cell_type":"markdown","id":"821c8ede-dd24-42e0-8636-6bd7d4704db4","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","# Measure prediction time for Random Forest\n","start_time_rf = time.time()\n","y_pred_rf = rf.predict(X_test)\n","end_time_rf = time.time()\n","rf_pred_time = end_time_rf - start_time_rf\n","\n","# Measure prediciton time for XGBoost\n","start_time_xgb = time.time()\n","y_pred_xgb = xgb.predict(X_test)\n","end_time_xgb = time.time()\n","xgb_pred_time = end_time_xgb - start_time_xgb\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"8588655f-db35-4f36-8d92-e691cad4fa6a","metadata":{},"outputs":[],"source":["### Exercise 3:  Calulate the MSE and R^2 values for both models\n"]},{"cell_type":"code","id":"a5e5bb22-6b76-4ff6-866e-1012af58f990","metadata":{},"outputs":[],"source":["# Enter your code here\n\nmse_rf = mean_squared_error(y_test, y_pred_xgb)\nmse_xgb = \nr2_rf = \nr2_xgb = "]},{"cell_type":"markdown","id":"4dfef139-0c76-46cb-8857-d5f4720a33e9","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n","r2_rf = r2_score(y_test, y_pred_rf)\n","r2_xgb = r2_score(y_test, y_pred_xgb)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"d86a72d8-04c9-4577-abaa-af01618258f4","metadata":{},"outputs":[],"source":["### Exercise 4:  Print the MSE and R^2 values for both models\n"]},{"cell_type":"code","id":"02c13cca-e2a8-490e-8800-571198911d10","metadata":{},"outputs":[],"source":["# Enter your code here\n\nprint(f'Random Forest:  MSE = {mse_rf:.4f}, R^2 = {...:.4f}')\nprint(f'      XGBoost:  ...')\n"]},{"cell_type":"markdown","id":"e9985011-2612-40d8-ae37-9414993c97cc","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","print(f'Random Forest:  MSE = {mse_rf:.4f}, R^2 = {r2_rf:.4f}')\n","print(f'      XGBoost:  MSE = {mse_xgb:.4f}, R^2 = {r2_xgb:.4f}')\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"e35f22ed-99e4-4714-8dbb-bf8c7331d46d","metadata":{},"outputs":[],"source":["You can see from the MSE and R^2 values that XGBoost is better than Random Forest, but the differences aren't overwhelming.\n"]},{"cell_type":"markdown","id":"c5a2e272-dc7e-400f-8da0-a5fb6da47255","metadata":{},"outputs":[],"source":["### Exercise 5:  Print the timings for each model\n"]},{"cell_type":"code","id":"7308be1e-85cd-4cb0-9294-ecf897c63189","metadata":{},"outputs":[],"source":["# Enter your code here\n\nprint(f'Random Forest:  Training Time = {...:.3f} seconds, Testing time = {...:.3f} seconds')\nprint(f'      XGBoost:  Training Time = {...:.3f} seconds, Testing time = {...:.3f} seconds')\n"]},{"cell_type":"markdown","id":"f3df84f6-9e46-4300-879d-9f08d82fb459","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","print(f'Random Forest:  Training Time = {rf_train_time:.3f} seconds, Testing time = {rf_pred_time:.3f} seconds')\n","print(f'      XGBoost:  Training Time = {xgb_train_time:.3f} seconds, Testing time = {xgb_pred_time:.3f} seconds')\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"08fd6da9-b932-43b3-8504-f5dc99683379","metadata":{},"outputs":[],"source":["What is very impressive is the difference in computation time between XGBoost and Random Forest for both training and testing.\n"]},{"cell_type":"markdown","id":"9c936b65-f97e-4a75-9021-8186a6db13aa","metadata":{},"outputs":[],"source":["Next, you want to generate scatter plots between the predicted and actual values for both models so you can visually evaluate how well each model performs.\n","We'll also plot lines one standard deviation of the test data above and below the ideal line, that is, the line that represents the perfect regressor, where the predictions are all correct.\n"]},{"cell_type":"markdown","id":"174e7ffc-7a4a-4588-9bca-42b15e00118c","metadata":{},"outputs":[],"source":["### Exercise 6. Calculate the standard deviation of the test data\n"]},{"cell_type":"code","id":"ad9241a6-ea6e-43ca-8a8c-4b41d6ed4baf","metadata":{},"outputs":[],"source":["# Enter your code here\nstd_y = np. ..."]},{"cell_type":"markdown","id":"cac1666c-d094-4e9f-876a-7b29f4e9c399","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","``` python\n","# Standard deviation of y_test\n","std_y = np.std(y_test)\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"f03e9168-439d-4cc3-829b-875cbc6e35f6","metadata":{},"outputs":[],"source":["start_time_rf = time.time()\ny_pred_rf = rf.predict(X_test)\nend_time_rf = time.time()\nrf_pred_time = end_time_rf - start_time_rf\n\n# Measure prediciton time for XGBoost\nstart_time_xgb = time.time()\ny_pred_xgb = xgb.predict(X_test)\nend_time_xgb = time.time()\nxgb_pred_time = end_time_xgb - start_time_xgb\n\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nmse_xgb = mean_squared_error(y_test, y_pred_xgb)\nr2_rf = r2_score(y_test, y_pred_rf)\nr2_xgb = r2_score(y_test, y_pred_xgb)\n\nprint(f'Random Forest:  MSE = {mse_rf:.4f}, R^2 = {r2_rf:.4f}')\nprint(f'      XGBoost:  MSE = {mse_xgb:.4f}, R^2 = {r2_xgb:.4f}')\nprint(f'Random Forest:  Training Time = {rf_train_time:.3f} seconds, Testing time = {rf_pred_time:.3f} seconds')\nprint(f'      XGBoost:  Training Time = {xgb_train_time:.3f} seconds, Testing time = {xgb_pred_time:.3f} seconds')\nstd_y = np.std(y_test)"]},{"cell_type":"markdown","id":"df8eda91-3ce7-47e3-a18d-a465925d3a49","metadata":{},"outputs":[],"source":["### Visualize the results\n"]},{"cell_type":"code","id":"697c5ba3-89ff-4eee-b42f-b17fca4f5b8c","metadata":{},"outputs":[],"source":["plt.figure(figsize=(14, 6))\n\n# Random Forest plot\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_rf, alpha=0.5, color=\"blue\",ec='k')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,label=\"perfect model\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min() + std_y, y_test.max() + std_y], 'r--', lw=1, label=\"+/-1 Std Dev\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min() - std_y, y_test.max() - std_y], 'r--', lw=1, )\nplt.ylim(0,6)\nplt.title(\"Random Forest Predictions vs Actual\")\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.legend()\n\n\n# XGBoost plot\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, y_pred_xgb, alpha=0.5, color=\"orange\",ec='k')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,label=\"perfect model\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min() + std_y, y_test.max() + std_y], 'r--', lw=1, label=\"+/-1 Std Dev\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min() - std_y, y_test.max() - std_y], 'r--', lw=1, )\nplt.ylim(0,6)\nplt.title(\"XGBoost Predictions vs Actual\")\nplt.xlabel(\"Actual Values\")\nplt.legend()\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"5b08a144-a85f-4a3f-a9c3-ec2651fc948d","metadata":{},"outputs":[],"source":["Both models performed very well. Most of their predictions fall within a standard deviation of the target. Interestingly, random forest \"respects\" the upper bound (the maximum value) present in the target by staying within its limits, while XGBoost \"overshoots\", or exceeds this limit. \n"]},{"cell_type":"markdown","id":"6b9fcdfc-a700-4c8c-b8b4-444ab9d934c8","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson.\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","\n","<!-- ## Changelog\n","\n","| Date | Version | Changed by | Change Description |\n","|:------------|:------|:------------------|:---------------------------------------|\n","| 2024-11-05 | 1.0  | Jeff Grossman    | Create content | -->\n","\n","\n","\n","## <h3 align=\"center\"> Â© IBM Corporation. All rights reserved. <h3/>\n"]},{"cell_type":"code","id":"cdd92092-b96f-4e34-8658-a29766f1f177","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"c0340e3960777afa61ba00feb3a189a8a1e26c7e0f9f9482c7c5c54268b84f9c"},"nbformat":4,"nbformat_minor":4}