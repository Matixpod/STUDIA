{"cells":[{"cell_type":"markdown","id":"9daee2c7-2361-4534-91a6-57915bbd28ce","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Lab: Applications of Principal Component Analysis (PCA)\n","Estimated time needed: **30** minutes\n","    \n","\n","## Objectives\n","\n","After completing this lab, you will be able to:\n","\n","* Use Principal Component Analysis (PCA) to project 2-D data onto its principal axes\n","* Use PCA for feature space dimensionality reduction\n","* Relate explained variance to feature importance and noise reduction\n","\n"]},{"cell_type":"markdown","id":"a73307c2-af3b-45b6-bbb2-ccbedfb31fda","metadata":{},"outputs":[],"source":["## Introduction\n","\n","In this lab you will explore how to implement two important applications of PCA.\n","\n","1. The first application illustrates how you can use PCA to project 2-D data onto its principal axes, meaning the two orthogonal directions that explain most of the variance in your data.\n","\n","2. For the second application, you will use PCA to project higher dimensional data down to a lower dimensional feature space. This is an example of dimension reduction, a powerful technique that has multiple benefits, including reducing your model-building computational load and, in many cases, the accuracy of your model. PCA can help you filter out redundant, linearly correlated variables and reduce the amount of noise in your data.\n"]},{"cell_type":"markdown","id":"d5691c96-98a3-4e5a-9627-c4d912327264","metadata":{},"outputs":[],"source":["# Part I: Using PCA to project 2-D data onto its principal axes\n","Here, you will illustrate how you can use PCA to transform your 2-D data to represent it in terms of its principal axes - the projection of your data onto the two orthogonal directions that explain most of the variance in your data. Let's see what all of this means.\n"]},{"cell_type":"markdown","id":"d13cb93e-0978-46d6-9875-1e13ff30a0d4","metadata":{},"outputs":[],"source":["Before you start, execute the cell below to make sure that the relevant libraries are available for use.\n"]},{"cell_type":"code","id":"6d6b8fde-1f55-4d9f-9ab5-34756b4ff1bc","metadata":{},"outputs":[],"source":["!pip install numpy==2.2.0\n!pip install scikit-learn==1.6.0\n!pip install matplotlib==3.9.3"]},{"cell_type":"markdown","id":"66d8b2d3-f47e-4d5f-bb91-2c6c29b2e2cc","metadata":{},"outputs":[],"source":["### Load libraries\n","Let's begin by importing the needed libraries.\n"]},{"cell_type":"code","id":"123bfe2d-5021-41de-ada8-fd62b3c40417","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","id":"3c52abe2-cef3-4383-83ff-7a8dbc87eb32","metadata":{},"outputs":[],"source":["### Create dataset\n","Next you'll create a 2-dimensional dataset containing two linearly correlated features.\n","\n","You'll use a bivariate normal distribution.\n","\n","Both features, X1 and X2, will have zero mean and a covariance given by the (symmetric) covariance matrix:\n","\n","\\begin{equation}\n","\\begin{pmatrix}\n","  3  & 2     \\\\\\\\\\\\\\\\\n","  2  & 2     \\\\\n","\\end{pmatrix}\n","\\end{equation}\n","\n","Here, the diagonal elements define the variances of X1 and X2 (3 and 2, respectively), while the off-diagonal element is the covariance (2) between X1 and X2, which expresses how similarly these features vary.\n"]},{"cell_type":"code","id":"f04f74f5-86d2-477d-8075-67f760589f89","metadata":{},"outputs":[],"source":["np.random.seed(42)\nmean = [0, 0]\ncov = [[3, 2], [2, 2]]\nX = np.random.multivariate_normal(mean=mean, cov=cov, size=200)"]},{"cell_type":"markdown","id":"eb337cc0-bded-4d52-8da9-c8a6378e0523","metadata":{},"outputs":[],"source":["### Exercise 1. Visualize the relationship between the two features.\n","For example, you can use a scatterplot.\n"]},{"cell_type":"code","id":"b4a74096-1382-482d-a6ac-c5d63d842cb8","metadata":{},"outputs":[],"source":["# enter your code here\n\n# Scatter plot of the two features\nplt.figure()\nplt.scatter(..., ..., edgecolor='k', alpha=0.7)\nplt.title(\"Scatter Plot of Bivariate Normal Distribution\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()"]},{"cell_type":"markdown","id":"8fea9886-e8e2-40f9-8d86-8a546558e0c8","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","# Scatter plot of the two features\n","plt.figure()\n","plt.scatter(X[:, 0], X[:, 1],  edgecolor='k', alpha=0.7)\n","plt.title(\"Scatter Plot of Bivariate Normal Distribution\")\n","plt.xlabel(\"X1\")\n","plt.ylabel(\"X2\")\n","plt.axis('equal')\n","plt.grid(True)\n","plt.show()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"fe7b7029-4259-4000-b814-8a723f99aa6f","metadata":{},"outputs":[],"source":["Consider the main direction the data follows in the scatterplot. It's actually the direction of the first principal component.\n","You can use PCA to determine this direction.\n","\n","###  Perform PCA on the dataset\n","Next, you'll initialize a 2-component PCA model with default parameters and then fit and transform the feature space in one step.\n"]},{"cell_type":"code","id":"c78b187a-37a9-4c86-9b37-36dd8f542082","metadata":{},"outputs":[],"source":["pca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)"]},{"cell_type":"markdown","id":"0fa739f0-a8f2-4fe5-a4d3-eddfa212f7c2","metadata":{},"outputs":[],"source":["### Get the principal components from the model. \n","The principal components are the principal axes, represented in feature space coordinates, \n","which align with the directions of maximum variance in your data. \n"]},{"cell_type":"code","id":"9137b3c3-5fcc-492b-9915-42199f6660d4","metadata":{},"outputs":[],"source":["components = pca.components_\ncomponents"]},{"cell_type":"markdown","id":"e9f91e5a-29d4-4d4e-b85c-ef5307d160e1","metadata":{},"outputs":[],"source":["The principal components are sorted in decreasing order by their explained variance, which can be expressed as a ratio:\n"]},{"cell_type":"code","id":"0608f00e-07e3-4006-9c59-5064e7c61d9a","metadata":{},"outputs":[],"source":["pca.explained_variance_ratio_"]},{"cell_type":"markdown","id":"b1369d27-6fce-43a1-8596-d8c20f0d716e","metadata":{},"outputs":[],"source":["### Exercise 2. What percentage of the variance in the data is explained by the first principal component?\n"]},{"cell_type":"markdown","id":"70474a58-7675-4a73-a164-3c36f273eab8","metadata":{},"outputs":[],"source":["Enter your answer here\n"]},{"cell_type":"markdown","id":"4a74943b-b174-4e57-a420-d3e742327a4c","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","You can see that the first component explains over 90% of the variance in the data, while the second explains about 9%.\n","</details>\n"]},{"cell_type":"markdown","id":"0a7da77a-6434-428b-b99b-c4b4432901e2","metadata":{},"outputs":[],"source":["### Display the results\n","Here, you'll use a scatterplot to display the data points in their original feature space, X1, X2.\n","\n","You'll also plot the projections of the data points onto their principal component directions.\n","\n","It's a bit technical, requiring some understanding of linear algebra, but the outcome will be instructive.\n","\n","Let's see how this works.\n","\n","#### Project the data onto its principal component axes\n","The projection of the data onto a given principal component yields the coordinates of each of the data points along that component's direction. \n","\n","The new coordinates are given by the dot products of each point's coordinates with the given PCA component. \n","\n","Specifically, the projections are given by:\n"]},{"cell_type":"code","id":"53f86b61-a6bb-4a28-b66b-a055576b401a","metadata":{},"outputs":[],"source":["projection_pc1 = np.dot(X, components[0])\nprojection_pc2 = np.dot(X, components[1])"]},{"cell_type":"markdown","id":"7f623a9a-235f-4609-801b-0ce01ca0cf3d","metadata":{},"outputs":[],"source":["Now that you have these coordinates, you can use them to represent the projections of each data point along the principal directions in the original feature space.\n","\n","In code:\n"]},{"cell_type":"code","id":"9cc95592-4499-4ab8-b4d0-53c7691df32c","metadata":{},"outputs":[],"source":["x_pc1 = projection_pc1 * components[0][0]\ny_pc1 = projection_pc1 * components[0][1]\nx_pc2 = projection_pc2 * components[1][0]\ny_pc2 = projection_pc2 * components[1][1]"]},{"cell_type":"markdown","id":"df6b6c16-ef1d-4d8f-862b-6bedb49aebd2","metadata":{},"outputs":[],"source":["#### Plot the results\n","Now let's visualize this.\n"]},{"cell_type":"code","id":"3101ec7d-ca0f-4199-a101-f446c7a668fe","metadata":{},"outputs":[],"source":["# Plot original data\nplt.figure()\nplt.scatter(X[:, 0], X[:, 1], label='Original Data', ec='k', s=50, alpha=0.6)\n\n# Plot the projections along PC1 and PC2\nplt.scatter(x_pc1, y_pc1, c='r', ec='k', marker='X', s=70, alpha=0.5, label='Projection onto PC 1')\nplt.scatter(x_pc2, y_pc2, c='b', ec='k', marker='X', s=70, alpha=0.5, label='Projection onto PC 2')\nplt.title('Linearly Correlated Data Projected onto Principal Components', )\nplt.xlabel('Feature 1',)\nplt.ylabel('Feature 2',)\nplt.legend()\nplt.grid(True)\nplt.axis('equal')\nplt.show()"]},{"cell_type":"markdown","id":"56968935-d598-4a54-9c16-47ccd8676f87","metadata":{},"outputs":[],"source":["It took some effort but now you can see what the the principal coordinates mean.\n","\n","The data varies in two main directions. \n","- The first direction, in red, is aligned in the direction having the widest variation.\n","\n","### Exercise 3. Describe the second direction.\n"]},{"cell_type":"markdown","id":"69047660-7925-4aaf-92fe-b38b00ae2dce","metadata":{},"outputs":[],"source":["Enter your answer here\n"]},{"cell_type":"markdown","id":"52c6767f-e28a-488b-ab11-0f4072c80748","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","The second direction, in blue, is perpendicular to first and has a lower variance.\n","</details>\n"]},{"cell_type":"markdown","id":"2e4e1d91-8d59-4fe3-8ba9-e9588871a751","metadata":{},"outputs":[],"source":["# Part II. PCA for feature space dimensionality reduction\n","For this second application, you'll use PCA to project the four-dimensional Iris feature data set down onto a two-dimensional feature space.\n","\n","This will have the added benefit of enabling you to visualize some of the most important structures in the dataset.\n"]},{"cell_type":"markdown","id":"1b19126f-069c-4d90-b643-7ff69d5ae088","metadata":{},"outputs":[],"source":["### Load and preprocess Iris data\n","Let's start by loading the iris data and standardizing is features.\n"]},{"cell_type":"code","id":"5ed1524d-5b8b-49f0-988c-2b5a78960675","metadata":{},"outputs":[],"source":["# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"]},{"cell_type":"markdown","id":"bff17ccc-4628-4a60-9da2-782a37f9cf33","metadata":{},"outputs":[],"source":["### Exercise 4. What are the Iris flower's names?\n"]},{"cell_type":"code","id":"1c2fe708-7f34-403a-9272-5b7804ca9506","metadata":{},"outputs":[],"source":["# Enter your code here"]},{"cell_type":"markdown","id":"dcd77ad1-b660-47e6-8ab8-a8d071d62ddf","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","iris.target_names\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"43b7c6a8-e0dc-45b4-af05-a348607b1e9f","metadata":{},"outputs":[],"source":["### Exercise 5. Initialize a PCA model and reduce the Iris data set dimensionality to two components\n"]},{"cell_type":"code","id":"8099b425-44cb-4d8e-a33e-314d9ca11890","metadata":{},"outputs":[],"source":["# Enter your code here\n# Apply PCA and reduce the dataset to 2 components\npca = PCA(n_components=2)\nX_pca = pca. ..."]},{"cell_type":"markdown","id":"f5cffe0c-1a3f-411a-bdd0-4fdf82bfbc27","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","# Apply PCA and reduce the dataset to 2 components\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"b7de87c3-4703-4b0e-8d23-801da0d02398","metadata":{},"outputs":[],"source":["# Plot the PCA-transformed data in 2D\nplt.figure(figsize=(8,6))\n\ncolors = ['navy', 'turquoise', 'darkorange']\nlw = 1\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, s=50, ec='k',alpha=0.7, lw=lw,\n                label=target_name)\n\nplt.title('PCA 2-dimensional reduction of IRIS dataset',)\nplt.xlabel(\"PC1\",)\nplt.ylabel(\"PC2\",)\nplt.legend(loc='best', shadow=False, scatterpoints=1,)\n# plt.grid(True)\nplt.show()"]},{"cell_type":"markdown","id":"7d6d0087-68cd-4a3a-a65a-cdcc2ddb17bf","metadata":{},"outputs":[],"source":["## Reflection\n","Examine the plot and consider how well the Iris classes have been separated simply by projecting the feature space down to two principal components.\n"]},{"cell_type":"markdown","id":"f494af89-b596-4b63-91b4-8de6ccad9db6","metadata":{},"outputs":[],"source":["### Exercise 6. What percentage of the original feature space variance do these two combined principal components explain?\n"]},{"cell_type":"code","id":"8fa73651-0b44-409d-830e-d05e78615bc6","metadata":{},"outputs":[],"source":["# Enter your code here\n\n# hint - add the individual values"]},{"cell_type":"markdown","id":"ba26decd-880d-4641-a1de-e6247cc27821","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","100*pca.explained_variance_ratio_.sum()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"748cd74c-ab42-4e97-8980-cafde4730e48","metadata":{},"outputs":[],"source":["## A deeper look at the explained variances\n","In this next and final set of exercises, your goal is to:\n","- Acquire and plot the PCA-explained variance ratios for all four Iris features as a barplot\n","- Overlay the cummulative explained variance\n","\n","### Exercise 7. Reinitialize the PCA model without reducing the dimension\n","Standardize the Iris data, and fit and transform the scaled data.\n"]},{"cell_type":"code","id":"b3f99ad2-6edc-45a4-b250-87cf83b0fcb4","metadata":{},"outputs":[],"source":["# Enter your code here\n\n# Standardize the data\nscaler = ... ()\nX_scaled = scaler.fit_transform(X)\n\n# Apply PCA\npca = ...\nX_pca = pca. ..."]},{"cell_type":"markdown","id":"dee2ea67-3e50-4c19-8819-8489d8e5244a","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply PCA\n","pca = PCA()\n","X_pca = pca.fit_transform(X_scaled)\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"ef11e6fc-cd51-446b-8422-fa3aea103ea3","metadata":{},"outputs":[],"source":["# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plot explained variance ratio for each component\nplt.figure(figsize=(10,6))\nplt.bar(x=range(1, len(explained_variance_ratio)+1), height=explained_variance_ratio, alpha=1, align='center', label='PC explained variance ratio' )\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance by Principal Components')\n\n# Plot cumulative explained variance\ncumulative_variance = np.cumsum(explained_variance_ratio)\nplt.step(range(1, 5), cumulative_variance, where='mid', linestyle='--', lw=3,color='red', label='Cumulative Explained Variance')\n# Only display integer ticks on the x-axis\nplt.xticks(range(1, 5))\nplt.legend()\nplt.grid(True)\nplt.show()"]},{"cell_type":"markdown","id":"43855672-c67b-445a-8b2a-28f556d8ccbb","metadata":{},"outputs":[],"source":["Here are some considerations:\n","- What does the red dashed line indicate to you?\n","- What would you consider doing if your wanted to suppress some noise in your data?\n"]},{"cell_type":"markdown","id":"72d1c33c-9327-4d75-97ef-2b7cc6301d7b","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson!\n","\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n","\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","\n","\n","<!-- ## Changelog\n","\n","| Date | Version | Changed by | Change Description |\n","|:------------|:------|:------------------|:---------------------------------------|\n","| 2024-11-05 | 1.0  | Jeff Grossman    | Create content  | -->\n","\n","\n","\n","## <h3 align=\"center\"> Â© IBM Corporation. All rights reserved. <h3/>\n"]},{"cell_type":"code","id":"62b1f0ed-8eaf-4a51-8e05-119779887da8","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"cea8b42b3b273ff7a6ee105c21a17a5148b2f1b77ca088edf174ef89872940ba"},"nbformat":4,"nbformat_minor":4}