import numpy as np
import matplotlib.pyplot as plt

class LogisticRegression:
    def __init__(self,epochs=1000,lr=0.01):
        self.w = None
        self.b = 0
        self.epochs = epochs
        self.lr = lr
    def fit(self,X,y):
        self.w = np.zeros(X.shape[1])
        for epoch in range(self.epochs):
            a = self.predict(X)
            print(f"Loss in epoch {epoch}: {self.binary_corss_entropy(y,a)}")
            error = a.reshape(-1) - y.reshape(-1)
            dw = (1/X.shape[0]) * (X.T @ error)
            db = (1/X.shape[0]) * np.sum(error)
            self.w = self.w - self.lr * dw
            self.b = self.b - self.lr * db
        return self.w, self.b
    
    def predict(self,X):
        z = X @ self.w + self.b
        a = self.sigmoid(z)
        return a
        
            
    def sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    def binary_corss_entropy(self,y_true,y_pred):
        epsilon = 1e-7
        y_pred = np.clip(y_pred, epsilon, 1-epsilon)
        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        return loss



def show(X,y,w,b):
    plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', label='Klasa 0')
    plt.scatter(X[y==1, 0], X[y==1, 1], color='red', label='Klasa 1')
    x1_vals = np.linspace(np.min(X[:,0]),np.max(X[:,1]), 100)
    x2_vals = -(w[0]*x1_vals + b) / w[1]
    plt.plot(x1_vals,x2_vals, 'g--', label='Decision Boundry')
    plt.show()
    
    
def main():
    np.random.seed(42)
    class_0 = np.random.randn(50,2) * 0.5 + [1,1]
    class_1= np.random.randn(50,2) * 0.5 + [2,2]
    X = np.vstack([class_0,class_1])
    y = np.hstack([np.zeros(50),np.ones(50)])

    model = LogisticRegression(epochs=10000)
    w,b = model.fit(X,y)
    print(f"test punktÃ³w:[1,1]: {(model.predict([1,1])>=0.5)}")
    show(X,y,w,b)

    

if __name__ == "__main__":
    main()








